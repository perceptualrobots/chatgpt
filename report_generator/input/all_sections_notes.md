# Technical Report - PCT Applied (with RL Comparison)
# Generated on: December 29, 2025 at 19:54:05
# Environment: Lunar Lander
# Focus: PCT primary; RL comparator baseline
================================================================================

## ABSTRACT
-----------

[Input file abstract.md not found]

================================================================================

## INTRODUCTION
---------------

# Introduction Notes

## Key Points

- **Control systems** are critical for autonomous agents
- **Traditional control** vs **modern AI approaches**
- **PCT** offers:
  - Biological inspiration
  - Interpretability
- **RL** provides data-driven learning capabilities
- **Research gap**: Direct comparison in standardized environment
- Specific environment provides consistent evaluation platform

================================================================================

## BACKGROUND
-------------

# Background Notes

## Topics to Cover

### Perceptual Control Theory
- **PCT fundamentals**: Simple and powerful hierarchical architecture
  - Self-correcting feedback loop
  - Adapts to environment \citep{powers1973} 

### Optimization & Learning
- **Evolutionary algorithms** for hierarchy optimization
- **Reinforcement learning** theory and deep Q-networks

### Environment
- **Characteristics and challenges**
- Reference: [Lunar Lander Environment](https://gymnasium.farama.org/environments/box2d/lunar_lander/)

### Related Work
- Previous comparative studies **limitations**
- Control system **evaluation metrics**

================================================================================

## METHODOLOGY
--------------

# Methodology Notes

## Environment Setup
- **Target environment**: Specify your target environment

## PCT Implementation
- **PCT hierarchy**: Optimally generated by evolutionary algorithm
  - Guided by rewards and specific fitness function
- **Evolutionary algorithm**: DEAP framework with Optuna hyperparameter optimization

## RL Baseline
- **RL approach**: Simphony taken from OpenAI Gym leaderboard
  - \citep{ishuov2024}

## Evaluation
- **Metrics**:
  - Episodes
  - Success rate (out of 100 retries)
  - Steps
  - Number of nodes
  - Number of weights

## Hardware
- **Specs**: This machine (CPU not GPU)

================================================================================

## EXPERIMENTAL RESULTS
-----------------------

# Experimental Results Notes

## Performance Data

### Computational Comparison

The RL controller implements a high-dimensional mapping between state and action an enormous network, relatively, is necessary as shown in the first image where the PCT controller is barely visible. The PCT network, shown in the second image, which dynamically adjusts action to maintain perceptual inputs is just 6 control units and has significantly fewer weights, by a factor of 10,000.

### Visual Media
- **Video** \citep{young2025} : Shows random controller, RL (Symphony) controller, and evolved PCT controller
- **Image**: `RLvPCT-toscale.png`
  - [width=1.0\textwidth]
  - Caption: The RL and PCT networks displayed to scale. The PCT controller is barely visible in comparison.
- **Image**: `PCT.png`
  - Caption: PCT network: 1 level with 6 control units.
  - 6 perceptions simultaneously controlled.
  - The Outputs combine to form environment actions

### Quantitative Results

**Performance comparison **: 100 episodes


| Metric | RL (Symphony) | PCT |
|--------|---------------|-----|
| Total Parameters | 335,622 | 29 |
| Total Nodes | 1,798 | 6 |
| Success Rate (count=100) | 75 | 79 |
| Failure Rate (count=-100) | 5 | 8 |
| Neutral Rate (count=0) | 20 | 13 |

Table: Comparative results for RL and PCT. A score of 100 indicates a successful landing, -100 a crash and 0 is incomplete landing at end of run.



## Presentation Elements
- Results summary table
- **Results reproduction**:
  - *TODO*: PCT example, link to code
  - Simphony model - \citep{ishuov2024}
- Videos
- Key findings and insights

================================================================================

## DISCUSSION
-------------

# Discussion Notes

## PCT Advantages
- **Interpretability**: Break down into control units
- **Biological plausibility**
- **Psychologically credible**
- **Smaller computational footprint**

## RL Advantages
- **Sample efficiency**
- **Generalization**
- **Scalability**

## Analysis Points
- **Comparative analysis**: Strengths and weaknesses of each approach
- **Trade-offs** between approaches
- **Implications** for real-world applications
- **Limitations** of current study
- **Unexpected findings** and their explanations

================================================================================

## RECOMMENDATIONS & FUTURE WORK
--------------------------------

# Recommendations & Future Work Notes

## Recommendations

### Hybrid Approaches
- Combining **PCT and RL**

### Extended Testing
- Testing on more **complex and realistic** world environments
- **Real-world robotics** applications

### Computational Optimization
- Implement **EPCT in deep learning framework**
- **Parallel processing** and GPUs

### AI Interpretability
- **Human-interpretable AI systems**

================================================================================

## REFERENCES
-------------

# References Notes

## Key References to Include

### Perceptual Control Theory

- Powers, W. T., Clark, R., and McFarland, R. (1960). A general feedback theory of human behavior: Part i. *Perceptual and motor skills*, 11(1):71–88.

- Powers, W. T. (1973). *Behavior: The control of perception*. Aldine de Gruyter.

- Young, R. (2017). A General Architecture for Robotics Systems: A Perception-Based Approach to Artificial Life. *Artificial Life*, 23(2):236–286.

- Young, R. (2020). Robotics in the real world: the perceptual control theory approach. In Mansell, W., editor, *The Interdisciplinary Handbook of Perceptual Control Theory*, chapter 14, pages 517–556. Academic Press.

### Reinforcement Learning

- Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction*. MIT Press.

- Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ... & Hassabis, D. (2015). Human-level control through deep reinforcement learning. *Nature*, 518(7540), 529-533.

### Implementation & Resources

- **Young, R. (2025)** Lunar Lander PCT v. RL  
  [https://www.youtube.com/watch?v=sxW8pNze1Ro](https://www.youtube.com/watch?v=sxW8pNze1Ro)

- **Ishuov, Timur  (2024)** Implementation of Lunar Lander with Symphony  
  - GitHub: [https://github.com/timurgepard/Simphony](https://github.com/timurgepard/Simphony)  
  - Video: [https://www.youtube.com/watch?v=7RA7GqHfdb0](https://www.youtube.com/watch?v=7RA7GqHfdb0)

================================================================================

