# Technical Report - PCT Applied (with RL Comparison)
# Generated on: December 20, 2025 at 20:16:52
# Environment: Lunar Lander
# Focus: PCT primary; RL comparator baseline
================================================================================

## ABSTRACT
-----------

[Input file abstract.txt not found]

================================================================================

## INTRODUCTION
---------------

Introduction notes:
- Control systems critical for autonomous agents
- Traditional control vs modern AI approaches
- PCT offers biological inspiration and interpretability
- RL provides data-driven learning capabilities
- Research gap: direct comparison in standardized environment
- Specific environment provides consistent evaluation platform

================================================================================

## BACKGROUND
-------------

Background information to cover:
- Perceptual Control Theory fundamentals - simple and powerful hierarchical architecure, self-correcting feedback loop, adapts to environment (Powers, 1973)
- Evolutionary algorithms for hierarchy optimization
- Reinforcement learning theory and deep Q-networks
- Environment characteristics and challenges - https://gymnasium.farama.org/environments/box2d/lunar_lander/
- Previous comparative studies limitations
- Control system evaluation metrics

================================================================================

## METHODOLOGY
--------------

Methodology details:
- Environment: Specify your target environment 
- PCT hierarchy: optimally generated by evolutionary algorithm, guided by rewards and specific fitness function. 
- Evolutionary algorithm: DEAP framework with Optuna hyperparameter optimization.
- RL approach: Simphony taken from OpenAI Gym leaderboard - include reference.
- Evaluation metrics: episodes, success of retries out of 100, steps, #nodes, #weights
- Hardware: spec of this machine (CPU not GPU)

================================================================================

## EXPERIMENTAL RESULTS
-----------------------

Results to present:

- Performance comparison across 100 episodes
- Video Young, R. (2025) shows random controller, RL (Symphony) controller and the evolved PCT controller.
- Image G:\My Drive\PR\reports\LunarLander\RLvPCT-toscale.png - shows the RL and PCT networks drawn to scale, with the PCT controller barely visible in comparison.
- Image G:\My Drive\PR\reports\LunarLander\PCT.png - the PCT network has just 1 level, with six control units. So, 6 perceptions simultaneously controlled, with their outputs combining to form the environment acions.


Summarise these results in a table:

- RL Result {"model_details": [{"name": "Actor", "total_parameters": 68610, "total_nodes": 514}, {"name": "Critic", "total_parameters": 267012, "total_nodes": 1284}], "total_parameters": 335622, "total_nodes": 1798, "num_episodes": 100, "count_100": 75, "count_neg100": 5, "count_near0": 20}
- PCT Result: {'model_details': {'total_nodes': 6, 'total_parameters': 29}, 'image_created': True, 'image_file': '/tmp/LunarLander_image.png', 'figure': <Figure size 800x800 with 1 Axes>, 'success': True}


- Results summary table
- Results reproduction
-- TODO: PCT example, link to code
-- Simphony model - include reference 
- Videos
- Key findings and insights

================================================================================

## DISCUSSION
-------------

Discussion points:
- PCT advantages: interpretability (break down into control units), biological plausibility, psychologically credible, smaller computational footprint
- RL advantages: sample efficiency, generalization, scalability
- Comparative analysis: strengths and weaknesses of each approach
- Trade-offs between approaches
- Implications for real-world applications
- Limitations of current study
- Unexpected findings and their explanations

================================================================================

## RECOMMENDATIONS & FUTURE WORK
--------------------------------

Recommendations and future work:
- Hybrid approaches combining PCT and RL
- Testing on more complex and realistic world environments
- Real-world robotics applications
- Computational optimization strategies - implement EPCT in deep learning framework, parallel processing and GPUs
- Human-interpretable AI systems

================================================================================

## REFERENCES
-------------

Key references to include:
- Powers, W. T., Clark, R., and McFarland, R. (1960). A general feedback theory of human behavior: Part i. Perceptual and motor skills, 11(1):71–88.
- Powers, W. T. (1973). Behavior: The control of perception. Aldine de Gruyter.
- Sutton, R. S., & Barto, A. G. (2018). Reinforcement Learning: An Introduction. MIT Press.
- Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness, J., Bellemare, M. G., ... & Hassabis, D. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529-533.
- Young, R. (2017). A General Architecture for Robotics Systems: A Perception-Based Approach to Artificial Life. Artificial Life, 23(2):236–286.
- Young, R. (2020). Robotics in the real world: the perceptual control theory approach. In Mansell, W., editor, The Interdisciplinary Handbook of Perceptual Control Theory, chapter 14, pages 517–556. Academic Press.
- Young, R. (2025) Lunar Lander PCT v. RL https://www.youtube.com/watch?v=sxW8pNze1Ro
- timurgepard (2025) Implemenation of Lunar Lander with Symphony. Github  https://github.com/timurgepard/Simphony, Video https://www.youtube.com/watch?v=7RA7GqHfdb0

================================================================================

